{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTHER(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length, start_token, batch_size, device='cpu'):\n",
    "        \n",
    "        super(OTHER, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Initialize on device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "\n",
    "        emb = self.embeddings(x)                    # [batch_size, sequence_length, embedding_dim]\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)   # lstm_out: [batch_size, sequence_length, hidden_dim]\n",
    "        logits = self.output_layer(lstm_out)        # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, num_samples):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Start token for all sequences\n",
    "            x = torch.full((num_samples, 1), self.start_token, dtype=torch.long, device=self.device)\n",
    "            hidden = None  # Let PyTorch initialize the hidden state\n",
    "\n",
    "            generated_sequences = torch.zeros(num_samples, self.sequence_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "            for i in range(self.sequence_length):\n",
    "                # Forward pass\n",
    "                emb = self.embeddings(x[:, -1:])  # Only use the last token\n",
    "                lstm_out, hidden = self.lstm(emb, hidden)\n",
    "                logits = self.output_layer(lstm_out)\n",
    "                \n",
    "                # Sample from distribution\n",
    "                probs = F.softmax(logits.squeeze(1), dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                \n",
    "                # Add to sequence\n",
    "                generated_sequences[:, i] = next_token.squeeze()\n",
    "                \n",
    "                # Update input for next step (only need the current token, not the entire history)\n",
    "                x = next_token\n",
    "            \n",
    "            return generated_sequences\n",
    "            \n",
    "    def calculate_nll(self, generated_sequences):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use all tokens except the last one as input\n",
    "            inputs = generated_sequences[:, :-1]\n",
    "            \n",
    "            # Use all tokens except the first one as targets\n",
    "            targets = generated_sequences[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self.forward(inputs)\n",
    "            \n",
    "            # Calculate negative log-likelihood\n",
    "            nll = F.cross_entropy(logits.reshape(-1, self.vocab_size), targets.reshape(-1), reduction='mean')\n",
    "            \n",
    "            return nll.item()\n",
    "            \n",
    "    def save_params(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def save_samples(self, samples, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            for sample in samples.cpu().numpy():\n",
    "                f.write(' '.join([str(int(x)) for x in sample]) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length, start_token, batch_size, device='cpu'):\n",
    "\n",
    "        np.random.seed(66)\n",
    "        torch.manual_seed(66)\n",
    "        \n",
    "        super(TargetLSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param, mean=0.0, std=0.1)\n",
    "        \n",
    "        # Initialize on device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "\n",
    "        emb = self.embeddings(x)                    # [batch_size, sequence_length, embedding_dim]\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)   # lstm_out: [batch_size, sequence_length, hidden_dim]\n",
    "        logits = self.output_layer(lstm_out)        # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, num_samples):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Start token for all sequences\n",
    "            x = torch.full((num_samples, 1), self.start_token, dtype=torch.long, device=self.device)\n",
    "            hidden = None  # Let PyTorch initialize the hidden state\n",
    "\n",
    "            generated_sequences = torch.zeros(num_samples, self.sequence_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "            for i in range(self.sequence_length):\n",
    "                # Forward pass\n",
    "                emb = self.embeddings(x[:, -1:])  # Only use the last token\n",
    "                lstm_out, hidden = self.lstm(emb, hidden)\n",
    "                logits = self.output_layer(lstm_out)\n",
    "                \n",
    "                # Sample from distribution\n",
    "                probs = F.softmax(logits.squeeze(1), dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                \n",
    "                # Add to sequence\n",
    "                generated_sequences[:, i] = next_token.squeeze()\n",
    "                \n",
    "                # Update input for next step (only need the current token, not the entire history)\n",
    "                x = next_token\n",
    "            \n",
    "            return generated_sequences\n",
    "            \n",
    "    def calculate_nll(self, generated_sequences):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use all tokens except the last one as input\n",
    "            inputs = generated_sequences[:, :-1]\n",
    "            \n",
    "            # Use all tokens except the first one as targets\n",
    "            targets = generated_sequences[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self.forward(inputs)\n",
    "            \n",
    "            # Calculate negative log-likelihood\n",
    "            nll = F.cross_entropy(logits.reshape(-1, self.vocab_size), targets.reshape(-1), reduction='mean')\n",
    "            \n",
    "            return nll.item()\n",
    "            \n",
    "    def save_params(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def save_samples(self, samples, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            for sample in samples.cpu().numpy():\n",
    "                f.write(' '.join([str(int(x)) for x in sample]) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters (matching the original implementation)\n",
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "hidden_dim = 32\n",
    "sequence_length = 20\n",
    "start_token = 0\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "oracle = TargetLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    sequence_length=sequence_length,\n",
    "    start_token=start_token,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (matching the original implementation)\n",
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "hidden_dim = 32\n",
    "sequence_length = 20\n",
    "start_token = 0\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "OTHER = TargetLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    sequence_length=sequence_length,\n",
    "    start_token=start_token,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 sequences of length 20\n",
      "Sequences shape: torch.Size([1000, 20])\n"
     ]
    }
   ],
   "source": [
    "# Generate some sequences using the Oracle\n",
    "num_samples = 1000\n",
    "oracle_sequences = oracle.generate(num_samples)\n",
    "print(f\"Generated {num_samples} sequences of length {sequence_length}\")\n",
    "print(f\"Sequences shape: {oracle_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLL of oracle-generated sequences: 8.512005805969238\n"
     ]
    }
   ],
   "source": [
    "oracle_nll = oracle.calculate_nll(oracle_sequences)\n",
    "print(f\"\\nNLL of oracle-generated sequences: {oracle_nll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL of random sequences: 8.511055946350098\n",
      "Difference: -0.000949859619140625\n"
     ]
    }
   ],
   "source": [
    "random_sequences = OTHER.generate(num_samples)\n",
    "random_nll = oracle.calculate_nll(random_sequences)\n",
    "print(f\"NLL of random sequences: {random_nll}\")\n",
    "print(f\"Difference: {random_nll - oracle_nll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL of random sequences: 8.522820472717285\n",
      "Difference: 0.010814666748046875\n"
     ]
    }
   ],
   "source": [
    "random_sequences = torch.randint(1, vocab_size, (num_samples, sequence_length), device=device)\n",
    "random_nll = oracle.calculate_nll(random_sequences)\n",
    "print(f\"NLL of random sequences: {random_nll}\")\n",
    "print(f\"Difference: {random_nll - oracle_nll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL of repeated token sequences: 8.521069526672363\n"
     ]
    }
   ],
   "source": [
    "repeated_sequences = torch.ones(num_samples, sequence_length, device=device, dtype=torch.long)\n",
    "for i in range(num_samples):\n",
    "    repeated_sequences[i] = i % 100 + 1  # Use tokens 1-100, repeating\n",
    "repeated_nll = oracle.calculate_nll(repeated_sequences)\n",
    "print(f\"NLL of repeated token sequences: {repeated_nll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
