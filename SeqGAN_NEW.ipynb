{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length, start_token, device='cuda'):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.device = device\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Output layer (maps hidden state to token logits)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass for training with teacher forcing\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_length]\n",
    "            hidden: Initial hidden state (optional)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits of shape [batch_size, seq_length, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            \n",
    "        # Embed input tokens\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # [batch_size, seq_length, hidden_dim]\n",
    "        \n",
    "        # Project to vocabulary space\n",
    "        logits = self.output_layer(lstm_out)  # [batch_size, seq_length, vocab_size]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).to(self.device)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def sample(self, batch_size, seq_length=None):\n",
    "        \"\"\"\n",
    "        Sample sequences from the generator\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of sequences to generate\n",
    "            seq_length: Length of sequences (defaults to self.sequence_length)\n",
    "            \n",
    "        Returns:\n",
    "            samples: Generated sequences of shape [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        if seq_length is None:\n",
    "            seq_length = self.sequence_length\n",
    "            \n",
    "        # Start with start tokens\n",
    "        samples = torch.full((batch_size, 1), self.start_token, \n",
    "                            dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Initial hidden state\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Generate one token at a time\n",
    "        for i in range(seq_length - 1):\n",
    "            # Get the last token\n",
    "            input_tokens = samples[:, -1].unsqueeze(1)\n",
    "            \n",
    "            # Embed it\n",
    "            embedded = self.embedding(input_tokens)\n",
    "            \n",
    "            # Get next hidden state and output\n",
    "            lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "            logits = self.output_layer(lstm_out.squeeze(1))\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Append to samples\n",
    "            samples = torch.cat([samples, next_token], dim=1)\n",
    "            \n",
    "        return samples\n",
    "    \n",
    "    def pretrain_step(self, x, optimizer):\n",
    "        \"\"\"\n",
    "        Supervised pre-training step using maximum likelihood\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences [batch_size, seq_length]\n",
    "            optimizer: PyTorch optimizer\n",
    "            \n",
    "        Returns:\n",
    "            loss: Training loss for this batch\n",
    "        \"\"\"\n",
    "        # Shift input and target\n",
    "        inp = x[:, :-1]\n",
    "        target = x[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = self.forward(inp)\n",
    "        logits = logits.contiguous().view(-1, self.vocab_size)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def reinforce_step(self, x, rewards, optimizer):\n",
    "        \"\"\"\n",
    "        Policy gradient update step\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences [batch_size, seq_length]\n",
    "            rewards: Rewards from rollout [batch_size, seq_length]\n",
    "            optimizer: PyTorch optimizer\n",
    "            \n",
    "        Returns:\n",
    "            loss: Policy gradient loss\n",
    "        \"\"\"\n",
    "        # Shift input and calculate log probs\n",
    "        inp = x[:, :-1]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = self.forward(inp)\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get the log probability of each chosen token\n",
    "        target = x[:, 1:].contiguous()\n",
    "        batch_size, seq_length = target.size()\n",
    "        \n",
    "        # Create one-hot encoding of targets\n",
    "        one_hot = torch.zeros(batch_size, seq_length, self.vocab_size).to(self.device)\n",
    "        one_hot.scatter_(2, target.unsqueeze(2), 1)\n",
    "        \n",
    "        # Calculate selected log probabilities and multiply by rewards\n",
    "        selected_log_probs = torch.sum(log_probs * one_hot, dim=-1)\n",
    "        rewards = rewards[:, 1:].contiguous()  # Align with targets\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        loss = -torch.sum(selected_log_probs * rewards) / batch_size\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/discriminator.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\"\"\"\n",
    "    def __init__(self, input_size, num_layers=1, bias=-2.0):\n",
    "        super(Highway, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.highway_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'transform': nn.Linear(input_size, input_size),\n",
    "                'gate': nn.Linear(input_size, input_size)\n",
    "            })\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Initialize gate bias to negative values to start with more carry behavior\n",
    "        for i in range(num_layers):\n",
    "            nn.init.constant_(self.highway_layers[i]['gate'].bias, self.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            transform = F.relu(self.highway_layers[i]['transform'](x))\n",
    "            gate = torch.sigmoid(self.highway_layers[i]['gate'](x))\n",
    "            x = gate * transform + (1 - gate) * x\n",
    "            \n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"CNN-based discriminator for sequence classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length, vocab_size, embedding_dim, filter_sizes, num_filters, dropout=0.5):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Convolutional layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n_filter, (filter_size, embedding_dim))\n",
    "            for filter_size, n_filter in zip(filter_sizes, num_filters)\n",
    "        ])\n",
    "        \n",
    "        # Highway network\n",
    "        self.total_filters = sum(num_filters)\n",
    "        self.highway = Highway(self.total_filters, num_layers=1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.total_filters, 2)  # Binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input sequences [batch_size, seq_length]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits of shape [batch_size, 2]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding Layer\n",
    "        x = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, seq_length, embedding_dim]\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            h = F.relu(conv(x))  # [batch_size, n_filter, seq_length - filter_size + 1, 1]\n",
    "            h = F.max_pool2d(h, (h.size(2), 1))  # [batch_size, n_filter, 1, 1]\n",
    "            h = h.squeeze(-1).squeeze(-1)  # [batch_size, n_filter]\n",
    "            conv_outputs.append(h)\n",
    "            \n",
    "        # Concatenate\n",
    "        concat_h = torch.cat(conv_outputs, dim=1)  # [batch_size, total_filters]\n",
    "        \n",
    "        # Highway layer\n",
    "        highway_out = self.highway(concat_h)\n",
    "        \n",
    "        # Dropout and fully-connected\n",
    "        dropped = self.dropout(highway_out)\n",
    "        logits = self.fc(dropped)\n",
    "        \n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
