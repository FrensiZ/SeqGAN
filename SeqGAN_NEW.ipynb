{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length, start_token, batch_size, device='cpu'):\n",
    "        \n",
    "        super(TargetLSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            nn.init.normal_(param, mean=0.0, std=0.1)\n",
    "        \n",
    "        # Initialize on device\n",
    "        self.to(device)\n",
    "       \n",
    "    def forward(self, x, hidden=None):\n",
    "\n",
    "        emb = self.embeddings(x)                    # [batch_size, sequence_length, embedding_dim]\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)   # lstm_out: [batch_size, sequence_length, hidden_dim]\n",
    "        logits = self.output_layer(lstm_out)        # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, num_samples):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Start token for all sequences\n",
    "            x = torch.full((num_samples, 1), self.start_token, dtype=torch.long, device=self.device)\n",
    "            hidden = None  # Let PyTorch initialize the hidden state\n",
    "\n",
    "            generated_sequences = torch.zeros(num_samples, self.sequence_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "            for i in range(self.sequence_length):\n",
    "                # Forward pass\n",
    "                emb = self.embeddings(x[:, -1:])  # Only use the last token\n",
    "                lstm_out, hidden = self.lstm(emb, hidden)\n",
    "                logits = self.output_layer(lstm_out)\n",
    "                \n",
    "                # Sample from distribution\n",
    "                probs = F.softmax(logits.squeeze(1), dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                \n",
    "                # Add to sequence\n",
    "                generated_sequences[:, i] = next_token.squeeze()\n",
    "                \n",
    "                # Update input for next step (only need the current token, not the entire history)\n",
    "                x = next_token\n",
    "            \n",
    "            return generated_sequences\n",
    "            \n",
    "    def calculate_nll(self, generated_sequences):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use all tokens except the last one as input\n",
    "            inputs = generated_sequences[:, :-1]\n",
    "            \n",
    "            # Use all tokens except the first one as targets\n",
    "            targets = generated_sequences[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self.forward(inputs)\n",
    "            \n",
    "            # Calculate negative log-likelihood\n",
    "            nll = F.cross_entropy(logits.reshape(-1, self.vocab_size), targets.reshape(-1), reduction='mean')\n",
    "            \n",
    "            return nll.item()\n",
    "\n",
    "    def load_params(self, params_path):\n",
    "        \"\"\"\n",
    "        Load parameters from a TensorFlow list format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(params_path, 'rb') as f:\n",
    "                try:\n",
    "                    params = pickle.load(f)\n",
    "                except UnicodeDecodeError:\n",
    "                    f.seek(0)\n",
    "                    params = pickle.load(f, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {str(e)}\")\n",
    "            return self\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 1. Embeddings\n",
    "            self.embeddings.weight.copy_(torch.tensor(params[0], dtype=torch.float32))\n",
    "            \n",
    "            # 2. LSTM Parameters\n",
    "            # Extract individual LSTM weights\n",
    "            Wi, Ui, bi = params[1], params[2], params[3]  # Input gate\n",
    "            Wf, Uf, bf = params[4], params[5], params[6]  # Forget gate\n",
    "            Wo, Uo, bo = params[7], params[8], params[9]  # Output gate\n",
    "            Wc, Uc, bc = params[10], params[11], params[12]  # Cell state\n",
    "            \n",
    "            # Concatenate the weights in PyTorch's expected format\n",
    "            weight_ih = np.vstack([Wi, Wf, Wc, Wo])\n",
    "            weight_hh = np.vstack([Ui, Uf, Uc, Uo])\n",
    "            \n",
    "            # Bias is also concatenated\n",
    "            bias_ih = np.concatenate([bi, bf, bc, bo])\n",
    "            bias_hh = np.zeros_like(bias_ih)\n",
    "            \n",
    "            # Copy to PyTorch model\n",
    "            self.lstm.weight_ih_l0.copy_(torch.tensor(weight_ih, dtype=torch.float32))\n",
    "            self.lstm.weight_hh_l0.copy_(torch.tensor(weight_hh, dtype=torch.float32))\n",
    "            self.lstm.bias_ih_l0.copy_(torch.tensor(bias_ih, dtype=torch.float32))\n",
    "            self.lstm.bias_hh_l0.copy_(torch.tensor(bias_hh, dtype=torch.float32))\n",
    "            \n",
    "            # 3. Output layer\n",
    "            self.output_layer.weight.copy_(torch.tensor(params[13].T, dtype=torch.float32))\n",
    "            self.output_layer.bias.copy_(torch.tensor(params[14], dtype=torch.float32))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def save_params(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def save_samples(self, samples, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            for sample in samples.cpu().numpy():\n",
    "                f.write(' '.join([str(int(x)) for x in sample]) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASK GPT why DataLoader is necessary and we not just put that as a function within the generator/discriminator classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)\n",
    "torch.manual_seed(66)\n",
    "\n",
    "# Hyperparameters (matching the original implementation)\n",
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "hidden_dim = 32\n",
    "sequence_length = 20\n",
    "start_token = 0\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "oracle = TargetLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    sequence_length=sequence_length,\n",
    "    start_token=start_token,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "oracle.load_params(params_path='save/target_params_py3.pkl')\n",
    "\n",
    "oracle_2 = TargetLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    sequence_length=sequence_length,\n",
    "    start_token=start_token,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "oracle_sequences = oracle.generate(num_samples)\n",
    "test_sequences = oracle_2.generate(num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLL of oracle-generated sequences: 6.061142921447754\n",
      "\n",
      "NLL of oracle-test sequences: 11.397992134094238\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nNLL of oracle-generated sequences: {oracle.calculate_nll(oracle_sequences)}\")\n",
    "print(f\"\\nNLL of oracle-test sequences: {oracle.calculate_nll(test_sequences)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
