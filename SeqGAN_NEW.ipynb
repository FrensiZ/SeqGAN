{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length, start_token, device='cpu'):\n",
    "        \n",
    "        super(TargetLSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Initialize on device\n",
    "        self.to(device)\n",
    "       \n",
    "    def forward(self, x, hidden=None):\n",
    "\n",
    "        emb = self.embeddings(x)                    # [batch_size, sequence_length, embedding_dim]\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)   # lstm_out: [batch_size, sequence_length, hidden_dim]\n",
    "        logits = self.output_layer(lstm_out)        # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, num_samples):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Start token for all sequences\n",
    "            x = torch.full((num_samples, 1), self.start_token, dtype=torch.long, device=self.device)\n",
    "            hidden = None  # Let PyTorch initialize the hidden state\n",
    "\n",
    "            generated_sequences = torch.zeros(num_samples, self.sequence_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "            for i in range(self.sequence_length):\n",
    "                # Forward pass\n",
    "                emb = self.embeddings(x[:, -1:])  # Only use the last token\n",
    "                lstm_out, hidden = self.lstm(emb, hidden)\n",
    "                logits = self.output_layer(lstm_out)\n",
    "                \n",
    "                # Sample from distribution\n",
    "                probs = F.softmax(logits.squeeze(1), dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                \n",
    "                # Add to sequence\n",
    "                generated_sequences[:, i] = next_token.squeeze()\n",
    "                \n",
    "                # Update input for next step (only need the current token, not the entire history)\n",
    "                x = next_token\n",
    "            \n",
    "            return generated_sequences\n",
    "            \n",
    "    def calculate_nll(self, generated_sequences):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use all tokens except the last one as input\n",
    "            inputs = generated_sequences[:, :-1]\n",
    "            \n",
    "            # Use all tokens except the first one as targets\n",
    "            targets = generated_sequences[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self.forward(inputs)\n",
    "            \n",
    "            # Calculate negative log-likelihood\n",
    "            nll = F.cross_entropy(logits.reshape(-1, self.vocab_size), targets.reshape(-1), reduction='mean')\n",
    "            \n",
    "            return nll.item()\n",
    "\n",
    "    def load_params(self, params_path):\n",
    "        \"\"\"\n",
    "        Load parameters from a TensorFlow list format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(params_path, 'rb') as f:\n",
    "                try:\n",
    "                    params = pickle.load(f)\n",
    "                except UnicodeDecodeError:\n",
    "                    f.seek(0)\n",
    "                    params = pickle.load(f, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {str(e)}\")\n",
    "            return self\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 1. Embeddings\n",
    "            self.embeddings.weight.copy_(torch.tensor(params[0], dtype=torch.float32))\n",
    "            \n",
    "            # 2. LSTM Parameters\n",
    "            # Extract individual LSTM weights\n",
    "            Wi, Ui, bi = params[1], params[2], params[3]  # Input gate\n",
    "            Wf, Uf, bf = params[4], params[5], params[6]  # Forget gate\n",
    "            Wo, Uo, bo = params[7], params[8], params[9]  # Output gate\n",
    "            Wc, Uc, bc = params[10], params[11], params[12]  # Cell state\n",
    "            \n",
    "            # Concatenate the weights in PyTorch's expected format\n",
    "            weight_ih = np.vstack([Wi, Wf, Wc, Wo])\n",
    "            weight_hh = np.vstack([Ui, Uf, Uc, Uo])\n",
    "            \n",
    "            # Bias is also concatenated\n",
    "            bias_ih = np.concatenate([bi, bf, bc, bo])\n",
    "            bias_hh = np.zeros_like(bias_ih)\n",
    "            \n",
    "            # Copy to PyTorch model\n",
    "            self.lstm.weight_ih_l0.copy_(torch.tensor(weight_ih, dtype=torch.float32))\n",
    "            self.lstm.weight_hh_l0.copy_(torch.tensor(weight_hh, dtype=torch.float32))\n",
    "            self.lstm.bias_ih_l0.copy_(torch.tensor(bias_ih, dtype=torch.float32))\n",
    "            self.lstm.bias_hh_l0.copy_(torch.tensor(bias_hh, dtype=torch.float32))\n",
    "            \n",
    "            # 3. Output layer\n",
    "            self.output_layer.weight.copy_(torch.tensor(params[13].T, dtype=torch.float32))\n",
    "            self.output_layer.bias.copy_(torch.tensor(params[14], dtype=torch.float32))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def save_params(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def save_samples(self, samples, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            for sample in samples.cpu().numpy():\n",
    "                f.write(' '.join([str(int(x)) for x in sample]) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length, start_token, device='cpu'):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "                \n",
    "        # Initialize on device\n",
    "        self.to(device)\n",
    "       \n",
    "    def forward(self, x, hidden=None):\n",
    "\n",
    "        emb = self.embeddings(x)                    # [batch_size, sequence_length, embedding_dim]\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)   # lstm_out: [batch_size, sequence_length, hidden_dim]\n",
    "        logits = self.output_layer(lstm_out)        # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, num_samples):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Start token for all sequences\n",
    "            x = torch.full((num_samples, 1), self.start_token, dtype=torch.long, device=self.device)\n",
    "            hidden = None  # Let PyTorch initialize the hidden state\n",
    "\n",
    "            generated_sequences = torch.zeros(num_samples, self.sequence_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "            for i in range(self.sequence_length):\n",
    "                # Forward pass\n",
    "                emb = self.embeddings(x[:, -1:])  # Only use the last token\n",
    "                lstm_out, hidden = self.lstm(emb, hidden)\n",
    "                logits = self.output_layer(lstm_out)\n",
    "                \n",
    "                # Sample from distribution\n",
    "                probs = F.softmax(logits.squeeze(1), dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                \n",
    "                # Add to sequence\n",
    "                generated_sequences[:, i] = next_token.squeeze()\n",
    "                \n",
    "                # Update input for next step (only need the current token, not the entire history)\n",
    "                x = next_token\n",
    "            \n",
    "            return generated_sequences\n",
    "        \n",
    "    def save_params(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def pretrain_step(self, x, optimizer):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs = x[:, :-1]                  # Forward pass - input is all tokens except last one\n",
    "        targets = x[:, 1:].contiguous()     # Target is all tokens except first one (shifted by 1)\n",
    "        \n",
    "        logits, _ = self.forward(inputs)\n",
    "    \n",
    "        loss = F.cross_entropy(logits.reshape(-1, self.vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "def pretrain_generator(target_lstm, generator, optimizer, pre_epoch_num, batch_size, generated_num, eval_freq, lr_patience, lr_decay):\n",
    "    \n",
    "    print('Start pre-training...')\n",
    "\n",
    "     # Open log file\n",
    "    log = open('NEW_experiment-log.txt', 'w')\n",
    "    log.write('pre-training...\\n')\n",
    "\n",
    "    # For learning rate scheduling\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "        \n",
    "    # Generate Oracle Data\n",
    "    #print('Generating data from oracle...')\n",
    "    oracle_data = target_lstm.generate(generated_num)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    oracle_dataset = torch.utils.data.TensorDataset(oracle_data)\n",
    "    oracle_loader = torch.utils.data.DataLoader(\n",
    "        oracle_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(pre_epoch_num):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Evaluate using the oracle every eval_freq epochs\n",
    "        if epoch % eval_freq == 0 or epoch == pre_epoch_num - 1:\n",
    "\n",
    "            generated_samples = generator.generate(generated_num)\n",
    "            \n",
    "            # Calculate NLL using the oracle\n",
    "            nll = target_lstm.calculate_nll(generated_samples)\n",
    "            print(f'Epoch {epoch}, NLL: {nll:.4f}')\n",
    "\n",
    "            # Log to file\n",
    "            buffer = f'epoch:\\t{epoch}\\tnll:\\t{nll:.5f}\\n'\n",
    "            log.write(buffer)\n",
    "            log.flush()  # Ensure it's written immediately\n",
    "        \n",
    "        # Train on all batches\n",
    "        for batch_data in oracle_loader:\n",
    "            x = batch_data[0]\n",
    "            loss = generator.pretrain_step(x, optimizer)\n",
    "            epoch_loss += loss\n",
    "            batch_count += 1\n",
    "        \n",
    "        # Calculate average loss for this epoch\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        #print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= lr_patience:\n",
    "            # Reduce learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= lr_decay\n",
    "            print(f\"Learning rate reduced to {optimizer.param_groups[0]['lr']}\")\n",
    "            patience_counter = 0\n",
    "\n",
    "    log.close()    \n",
    "    print('Pretraining finished!')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training...\n",
      "Epoch 0, NLL: 11.4096\n",
      "Epoch 5, NLL: 10.2834\n",
      "Epoch 10, NLL: 10.2727\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "VOCAB_SIZE = 5000\n",
    "EMB_DIM = 32 \n",
    "HIDDEN_DIM = 32 \n",
    "SEQ_LENGTH = 20 \n",
    "START_TOKEN = 0\n",
    "PRE_EPOCH_NUM = 300\n",
    "BATCH_SIZE = 64\n",
    "SEED = 88\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generated_num = 10000\n",
    "\n",
    "# Create models\n",
    "target_lstm = TargetLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, device)\n",
    "target_lstm.load_params(params_path='save/target_params_py3.pkl')\n",
    "generator = Generator(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "\n",
    "# PRETRAINING\n",
    "pretrain_generator(target_lstm, generator, optimizer, PRE_EPOCH_NUM, BATCH_SIZE, generated_num, eval_freq=5, lr_patience=5, lr_decay=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
