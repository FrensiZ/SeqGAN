{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Make sure this is imported\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenDataLoader:\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.token_stream = []\n",
    "        self.num_batch = 0\n",
    "        self.pointer = 0\n",
    "        self.sequence_batch = []\n",
    "\n",
    "    def create_batches(self, data_file):\n",
    "        self.token_stream = []\n",
    "        with open(data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                if len(parse_line) == 20:  # Fixed sequence length of 20\n",
    "                    self.token_stream.append(parse_line)\n",
    "\n",
    "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
    "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
    "        self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sequence_batch[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "class DisDataloader:\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "        self.num_batch = 0\n",
    "        self.pointer = 0\n",
    "        self.sentences_batches = []\n",
    "        self.labels_batches = []\n",
    "\n",
    "    def load_train_data(self, positive_file, negative_file):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "\n",
    "        with open(positive_file) as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                positive_examples.append(parse_line)\n",
    "\n",
    "        with open(negative_file) as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                if len(parse_line) == 20:\n",
    "                    negative_examples.append(parse_line)\n",
    "\n",
    "        self.sentences = np.array(positive_examples + negative_examples)\n",
    "\n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "        # Shuffle the data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
    "        self.sentences = self.sentences[shuffle_indices]\n",
    "        self.labels = self.labels[shuffle_indices]\n",
    "\n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, \n",
    "                 sequence_length, start_token, learning_rate=0.01, reward_gamma=0.95):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.num_emb = num_emb  # Vocabulary size\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reward_gamma = reward_gamma\n",
    "        self.temperature = 1.0\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(num_emb, emb_dim)\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Output layer: maps hidden state to vocabulary distribution\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_emb)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def init_hidden(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        return (h, c)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the generator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            hidden: Initial hidden state tuple (h, c)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits [batch_size, seq_len, vocab_size]\n",
    "            hidden: Final hidden state\n",
    "        \"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "            \n",
    "        # Get embeddings for input tokens\n",
    "        emb = self.embeddings(x)  # [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # Process through LSTM\n",
    "        output, hidden = self.lstm(emb, hidden)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Map to vocabulary space\n",
    "        logits = self.output_layer(output)  # [batch_size, seq_len, num_emb]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def sample(self, num_samples=None, hidden=None):\n",
    "        \"\"\"\n",
    "        Sample a batch of sequences from the generator.\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of samples to generate (defaults to batch_size)\n",
    "            hidden: Initial hidden state\n",
    "            \n",
    "        Returns:\n",
    "            generated_samples: Tensor of token indices [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.batch_size\n",
    "            \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(num_samples)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            device = next(self.parameters()).device\n",
    "            # Start with start token for each sequence\n",
    "            x = torch.full((num_samples,), self.start_token, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Store generated tokens\n",
    "            generated_samples = torch.zeros(num_samples, self.sequence_length, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Generate tokens one at a time\n",
    "            for i in range(self.sequence_length):\n",
    "                # Get embeddings for current token\n",
    "                emb = self.embeddings(x).unsqueeze(1)  # [batch_size, 1, emb_dim]\n",
    "                \n",
    "                # Process through LSTM\n",
    "                output, hidden = self.lstm(emb, hidden)  # [batch_size, 1, hidden_dim]\n",
    "                \n",
    "                # Get logits\n",
    "                logits = self.output_layer(output.squeeze(1))  # [batch_size, num_emb]\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "                x = torch.multinomial(probs, 1).squeeze()\n",
    "                \n",
    "                # Store the generated token\n",
    "                generated_samples[:, i] = x\n",
    "                \n",
    "            return generated_samples\n",
    "    \n",
    "    def pretrain_step(self, x):\n",
    "        \"\"\"\n",
    "        Perform one step of maximum likelihood pretraining.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss: The cross-entropy loss\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare inputs and targets\n",
    "        # Input: all tokens except the last one\n",
    "        inputs = x[:, :-1]\n",
    "        # Target: all tokens except the first one (which is <start>)\n",
    "        targets = x[:, 1:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = self.forward(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits.reshape(-1, self.num_emb), targets.reshape(-1))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def adversarial_loss(self, x, rewards):\n",
    "        \"\"\"\n",
    "        Compute the policy gradient loss for adversarial training.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            rewards: Rewards for each token [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss: The policy gradient loss\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare inputs and targets\n",
    "        # Input: all tokens except the last one\n",
    "        inputs = x[:, :-1]\n",
    "        # Target: all tokens except the first one (which is <start>)\n",
    "        targets = x[:, 1:]\n",
    "        # Rewards: align with targets\n",
    "        rewards = rewards[:, 1:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = self.forward(inputs)\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # One-hot encode targets\n",
    "        target_one_hot = F.one_hot(targets, num_classes=self.num_emb).float()\n",
    "        \n",
    "        # Compute token-level rewards\n",
    "        token_rewards = torch.sum(log_probs * target_one_hot, dim=-1) * rewards\n",
    "        \n",
    "        # Policy gradient loss (negative expected reward)\n",
    "        loss = -torch.mean(token_rewards)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "\n",
    "    def __init__(self, size, num_layers=1, bias=-2.0):\n",
    "        super(Highway, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.highways = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'transform': nn.Linear(size, size),\n",
    "                'gate': nn.Linear(size, size)\n",
    "            })\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            transform = F.relu(self.highways[i]['transform'](x))\n",
    "            gate = torch.sigmoid(self.highways[i]['gate'](x) + self.bias)\n",
    "            x = gate * transform + (1.0 - gate) * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, \n",
    "                 embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0, dropout_keep_prob=0.75):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Create multiple convolutional layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filter, [filter_size, embedding_size], padding=(0, 0))\n",
    "            for filter_size, num_filter in zip(filter_sizes, num_filters)\n",
    "        ])\n",
    "        \n",
    "        # Highway layer\n",
    "        self.highway = Highway(sum(num_filters), num_layers=1, bias=0)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(1.0 - dropout_keep_prob)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-4, weight_decay=l2_reg_lambda)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the discriminator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits [batch_size, num_classes]\n",
    "            probs: Output probabilities [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Get embeddings for input tokens\n",
    "        emb = self.embeddings(x)  # [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # Add channel dimension for CNN\n",
    "        emb = emb.unsqueeze(1)  # [batch_size, 1, seq_len, emb_dim]\n",
    "        \n",
    "        # Apply convolutions and max-pooling\n",
    "        conv_outputs = []\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # Convolution\n",
    "            h = F.relu(conv(emb))  # [batch_size, num_filters[i], seq_len-filter_sizes[i]+1, 1]\n",
    "            \n",
    "            # Max-pooling over time\n",
    "            pooled = F.max_pool2d(h, (h.size(2), 1))  # [batch_size, num_filters[i], 1, 1]\n",
    "            \n",
    "            # Flatten\n",
    "            pooled = pooled.squeeze(3).squeeze(2)  # [batch_size, num_filters[i]]\n",
    "            \n",
    "            conv_outputs.append(pooled)\n",
    "        \n",
    "        # Concatenate all conv outputs\n",
    "        h_pool_flat = torch.cat(conv_outputs, dim=1)  # [batch_size, sum(num_filters)]\n",
    "        \n",
    "        # Apply highway network\n",
    "        h_highway = self.highway(h_pool_flat)\n",
    "        \n",
    "        # Apply dropout\n",
    "        h_drop = self.dropout(h_highway)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        logits = self.fc(h_drop)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        return logits, probs\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"\n",
    "        Perform one training step.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            y: Target tensor of labels [batch_size, num_classes]\n",
    "            \n",
    "        Returns:\n",
    "            loss: The cross-entropy loss\n",
    "            probs: The output probabilities\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, probs = self.forward(x)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, torch.argmax(y, dim=1))\n",
    "        \n",
    "        # Add L2 regularization\n",
    "        if self.l2_reg_lambda > 0:\n",
    "            l2_loss = 0\n",
    "            for param in self.parameters():\n",
    "                l2_loss += torch.norm(param, 2)\n",
    "            loss += self.l2_reg_lambda * l2_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item(), probs.detach()  # Return detached probability tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout(nn.Module):\n",
    "    \"\"\"\n",
    "    Rollout module for Monte Carlo search in SeqGAN.\n",
    "    This creates a copy of the generator and performs rollouts to estimate rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, update_rate):\n",
    "        super(Rollout, self).__init__()\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "        # Copy parameters from the generator\n",
    "        self.own_generator = Generator(\n",
    "            generator.num_emb,\n",
    "            generator.batch_size,\n",
    "            generator.emb_dim,\n",
    "            generator.hidden_dim,\n",
    "            generator.sequence_length,\n",
    "            generator.start_token,\n",
    "            generator.learning_rate,\n",
    "            generator.reward_gamma\n",
    "        )\n",
    "        \n",
    "        # Copy all parameters from original generator\n",
    "        self.update_params()\n",
    "    \n",
    "    def update_params(self):\n",
    "        \"\"\"\n",
    "        Update the parameters of the rollout generator using the original generator.\n",
    "        In the original TF implementation, this was done with an update rate.\n",
    "        In PyTorch, we simply copy all parameters.\n",
    "        \"\"\"\n",
    "        for target_param, source_param in zip(self.own_generator.parameters(), self.generator.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.update_rate * target_param.data + (1.0 - self.update_rate) * source_param.data\n",
    "            )\n",
    "    \n",
    "    def get_reward(self, x, rollout_num, discriminator):\n",
    "        \"\"\"\n",
    "        Get reward for each token in the sequences using rollouts with the discriminator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences [batch_size, seq_len]\n",
    "            rollout_num: Number of rollouts to perform\n",
    "            discriminator: The discriminator model\n",
    "            \n",
    "        Returns:\n",
    "            rewards: Reward for each token in each sequence [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = self.generator.sequence_length\n",
    "        rewards = torch.zeros(batch_size, seq_len).to(x.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            discriminator.eval()\n",
    "            self.own_generator.eval()\n",
    "            \n",
    "            # Evaluate final reward (complete sequences) using discriminator\n",
    "            logits, probs = discriminator(x)\n",
    "            rewards[:, -1] = probs[:, 1]  # Probability of being real for completed sequence\n",
    "            \n",
    "            # For each position in the sequence\n",
    "            for given_num in range(1, seq_len):\n",
    "                # Store intermediate rollout rewards\n",
    "                position_rewards = torch.zeros(batch_size).to(x.device)\n",
    "                \n",
    "                # Perform multiple rollouts\n",
    "                for _ in range(rollout_num):\n",
    "                    # Continue sequence generation from this position\n",
    "                    rollout_samples = self._rollout_from_position(x, given_num)\n",
    "                    \n",
    "                    # Get reward from discriminator for the rolled-out sequences\n",
    "                    logits, probs = discriminator(rollout_samples)\n",
    "                    position_rewards += probs[:, 1]  # Accumulate probability of being real\n",
    "                \n",
    "                # Average rewards across rollouts\n",
    "                rewards[:, given_num-1] = position_rewards / rollout_num\n",
    "            \n",
    "            # Apply reward discount\n",
    "            if self.generator.reward_gamma < 1.0:\n",
    "                for i in range(seq_len - 1):\n",
    "                    rewards[:, i] = rewards[:, i] * self.generator.reward_gamma\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def _rollout_from_position(self, x, given_num):\n",
    "        \"\"\"\n",
    "        Generate rollout sequences starting from a given position.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences [batch_size, seq_len]\n",
    "            given_num: Position up to which to use original sequence\n",
    "            \n",
    "        Returns:\n",
    "            rollout_samples: Rolled-out sequences [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Create a copy of the input sequence\n",
    "        rollout_samples = x.clone()\n",
    "        \n",
    "        # Initialize hidden state by running the prefix through the LSTM\n",
    "        hidden = self.own_generator.init_hidden(batch_size)\n",
    "        \n",
    "        # First run the prefix through LSTM to get the hidden state at given_num\n",
    "        prefix = x[:, :given_num]\n",
    "        prefix_emb = self.own_generator.embeddings(prefix)\n",
    "        _, hidden = self.own_generator.lstm(prefix_emb, hidden)\n",
    "        \n",
    "        # Then generate tokens from given_num to the end\n",
    "        current = x[:, given_num-1]\n",
    "        \n",
    "        for i in range(given_num, self.generator.sequence_length):\n",
    "            # Get embeddings for current token\n",
    "            emb = self.own_generator.embeddings(current).unsqueeze(1)  # [batch_size, 1, emb_dim]\n",
    "            \n",
    "            # Process through LSTM\n",
    "            output, hidden = self.own_generator.lstm(emb, hidden)  # [batch_size, 1, hidden_dim]\n",
    "            \n",
    "            # Get logits\n",
    "            logits = self.own_generator.output_layer(output.squeeze(1))  # [batch_size, num_emb]\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(logits / self.own_generator.temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).squeeze()\n",
    "            \n",
    "            # Store the generated token\n",
    "            rollout_samples[:, i] = next_token\n",
    "            current = next_token\n",
    "        \n",
    "        return rollout_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Oracle LSTM model used for synthetic data experiments.\n",
    "    This model should be pre-trained and fixed during SeqGAN training.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token):\n",
    "        super(TargetLSTM, self).__init__()\n",
    "        \n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.temperature = 1.0\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = nn.Embedding(num_emb, emb_dim)\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Output layer: maps hidden state to vocabulary distribution\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_emb)\n",
    "    \n",
    "    def init_hidden(self, batch_size=None):\n",
    "        \"\"\"Initialize hidden state and cell state for LSTM\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "            \n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        return (h, c)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the target LSTM.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            hidden: Initial hidden state tuple (h, c)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits [batch_size, seq_len, vocab_size]\n",
    "            hidden: Final hidden state\n",
    "        \"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(x.size(0))\n",
    "            \n",
    "        # Get embeddings for input tokens\n",
    "        emb = self.embeddings(x)  # [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # Process through LSTM\n",
    "        output, hidden = self.lstm(emb, hidden)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Map to vocabulary space\n",
    "        logits = self.output_layer(output)  # [batch_size, seq_len, num_emb]\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def sample(self, num_samples=None, hidden=None):\n",
    "        \"\"\"\n",
    "        Sample a batch of sequences from the target LSTM.\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of samples to generate (defaults to batch_size)\n",
    "            hidden: Initial hidden state\n",
    "            \n",
    "        Returns:\n",
    "            generated_samples: Tensor of token indices [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        if num_samples is None:\n",
    "            num_samples = self.batch_size\n",
    "            \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(num_samples)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            device = next(self.parameters()).device\n",
    "            # Start with start token for each sequence\n",
    "            x = torch.full((num_samples,), self.start_token, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Store generated tokens\n",
    "            generated_samples = torch.zeros(num_samples, self.sequence_length, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Generate tokens one at a time\n",
    "            for i in range(self.sequence_length):\n",
    "                # Get embeddings for current token\n",
    "                emb = self.embeddings(x).unsqueeze(1)  # [batch_size, 1, emb_dim]\n",
    "                \n",
    "                # Process through LSTM\n",
    "                output, hidden = self.lstm(emb, hidden)  # [batch_size, 1, hidden_dim]\n",
    "                \n",
    "                # Get logits\n",
    "                logits = self.output_layer(output.squeeze(1))  # [batch_size, num_emb]\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                probs = F.softmax(logits / self.temperature, dim=-1)\n",
    "                x = torch.multinomial(probs, 1).squeeze()\n",
    "                \n",
    "                # Store the generated token\n",
    "                generated_samples[:, i] = x\n",
    "                \n",
    "            return generated_samples\n",
    "    \n",
    "    def pretrain_loss(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the negative log-likelihood loss for the target LSTM.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss: The cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Prepare inputs and targets\n",
    "        # Input: all tokens except the last one\n",
    "        inputs = x[:, :-1]\n",
    "        # Target: all tokens except the first one (which is <start>)\n",
    "        targets = x[:, 1:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = self.forward(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits.reshape(-1, self.num_emb), targets.reshape(-1))\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def init_model(self, params=None):\n",
    "        \"\"\"\n",
    "        Initialize the model with specific parameters to create a fixed oracle.\n",
    "        \n",
    "        Args:\n",
    "            params: Dictionary of pre-defined weights or list of tensors\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            return\n",
    "            \n",
    "        if isinstance(params, dict):\n",
    "            # If params is a dictionary with the expected structure\n",
    "            if 'emb' in params:\n",
    "                self.embeddings.weight.data = params['emb']\n",
    "                \n",
    "            if 'lstm' in params:\n",
    "                if isinstance(params['lstm'], dict):\n",
    "                    # Modern format with named parameters\n",
    "                    for layer, param_dict in params['lstm'].items():\n",
    "                        if isinstance(param_dict, dict):\n",
    "                            for param_name, param_tensor in param_dict.items():\n",
    "                                # Set the appropriate LSTM parameters\n",
    "                                getattr(self.lstm, param_name + '_' + layer).data = param_tensor\n",
    "                        else:\n",
    "                            # Direct tensor assignment\n",
    "                            setattr(self.lstm, layer, param_dict)\n",
    "                else:\n",
    "                    # Old format or different structure\n",
    "                    print(\"Unrecognized LSTM parameter format.\")\n",
    "                    \n",
    "            if 'out_w' in params:\n",
    "                self.output_layer.weight.data = params['out_w']\n",
    "                \n",
    "            if 'out_b' in params:\n",
    "                self.output_layer.bias.data = params['out_b']\n",
    "        elif isinstance(params, list):\n",
    "            # If params is a list of tensors (original TF implementation format)\n",
    "            try:\n",
    "                # Embeddings\n",
    "                self.embeddings.weight.data = params[0]\n",
    "                \n",
    "                # LSTM parameters - map to PyTorch's format\n",
    "                # This is a simplified mapping and may need adjustments\n",
    "                ih_weights = torch.cat([params[1], params[4], params[7], params[10]], dim=0)\n",
    "                hh_weights = torch.cat([params[2], params[5], params[8], params[11]], dim=0)\n",
    "                ih_bias = torch.cat([params[3], params[6], params[9], params[12]], dim=0)\n",
    "                \n",
    "                self.lstm.weight_ih_l0.data = ih_weights\n",
    "                self.lstm.weight_hh_l0.data = hh_weights\n",
    "                self.lstm.bias_ih_l0.data = ih_bias\n",
    "                self.lstm.bias_hh_l0.data = torch.zeros_like(ih_bias)\n",
    "                \n",
    "                # Output layer\n",
    "                self.output_layer.weight.data = params[13]\n",
    "                self.output_layer.bias.data = params[14]\n",
    "                \n",
    "                print(\"Initialized target LSTM from tensor list format.\")\n",
    "            except (IndexError, ValueError) as e:\n",
    "                print(f\"Error initializing from tensor list: {e}\")\n",
    "        else:\n",
    "            print(f\"Unsupported parameter format: {type(params)}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SeqGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training and evaluation\n",
    "\n",
    "def generate_samples(model, batch_size, generated_num, output_file, device):\n",
    "    \"\"\"\n",
    "    Generate samples using the model and save to file.\n",
    "    \n",
    "    Args:\n",
    "        model: Generator or TargetLSTM model\n",
    "        batch_size: Batch size for generation\n",
    "        generated_num: Total number of samples to generate\n",
    "        output_file: File to save the generated samples\n",
    "        device: Device to run the model on\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate batches of samples\n",
    "        for _ in range(int(generated_num / batch_size)):\n",
    "            samples = model.sample(batch_size)\n",
    "            # Convert to numpy and append to list\n",
    "            generated_samples.extend(samples.cpu().numpy().tolist())\n",
    "    \n",
    "    # Write samples to file\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for sample in generated_samples:\n",
    "            buffer = ' '.join([str(x) for x in sample]) + '\\n'\n",
    "            fout.write(buffer)\n",
    "\n",
    "\n",
    "def target_loss(target_lstm, data_loader, device):\n",
    "    \"\"\"\n",
    "    Calculate the negative log-likelihood of data according to target_lstm.\n",
    "    \n",
    "    Args:\n",
    "        target_lstm: The oracle LSTM model\n",
    "        data_loader: Data loader containing batches to evaluate\n",
    "        device: Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        avg_loss: Average negative log-likelihood\n",
    "    \"\"\"\n",
    "    target_lstm.eval()\n",
    "    total_loss = 0.0\n",
    "    data_loader.reset_pointer()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(data_loader.num_batch):\n",
    "            batch = data_loader.next_batch()\n",
    "            batch_tensor = torch.LongTensor(batch).to(device)\n",
    "            loss = target_lstm.pretrain_loss(batch_tensor)\n",
    "            total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / data_loader.num_batch\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def pre_train_epoch(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Pre-train the generator for one epoch using maximum likelihood.\n",
    "    \n",
    "    Args:\n",
    "        model: The generator model\n",
    "        data_loader: Data loader containing real data batches\n",
    "        device: Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        avg_loss: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    data_loader.reset_pointer()\n",
    "    \n",
    "    for _ in range(data_loader.num_batch):\n",
    "        batch = data_loader.next_batch()\n",
    "        batch_tensor = torch.LongTensor(batch).to(device)\n",
    "        loss = model.pretrain_step(batch_tensor)\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / data_loader.num_batch\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_discriminator(discriminator, data_loader, device, dropout_keep_prob=0.75):\n",
    "    \"\"\"\n",
    "    Evaluate the discriminator on a dataset without updating weights.\n",
    "    \n",
    "    Args:\n",
    "        discriminator: The discriminator model\n",
    "        data_loader: Data loader containing evaluation data\n",
    "        device: Device to run the model on\n",
    "        dropout_keep_prob: Dropout keep probability\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    discriminator.eval()\n",
    "    data_loader.reset_pointer()\n",
    "    \n",
    "    d_loss_sum = 0.0\n",
    "    real_probs_sum = 0.0\n",
    "    fake_probs_sum = 0.0\n",
    "    real_count = 0\n",
    "    fake_count = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(data_loader.num_batch):\n",
    "            x_batch, y_batch = data_loader.next_batch()\n",
    "            x_tensor = torch.LongTensor(x_batch).to(device)\n",
    "            y_tensor = torch.FloatTensor(y_batch).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, probs = discriminator(x_tensor)\n",
    "            loss = F.cross_entropy(logits, torch.argmax(y_tensor, dim=1))\n",
    "            \n",
    "            # Process probabilities\n",
    "            real_probs = probs[:, 1].cpu().numpy()  # Already detached in torch.no_grad()\n",
    "            real_indices = np.where(y_batch[:, 1] == 1)[0]  # Indices of real samples\n",
    "            fake_indices = np.where(y_batch[:, 0] == 1)[0]  # Indices of fake samples\n",
    "            \n",
    "            if len(real_indices) > 0:\n",
    "                real_probs_sum += np.sum(real_probs[real_indices])\n",
    "                real_count += len(real_indices)\n",
    "            \n",
    "            if len(fake_indices) > 0:\n",
    "                fake_probs_sum += np.sum(real_probs[fake_indices])\n",
    "                fake_count += len(fake_indices)\n",
    "            \n",
    "            d_loss_sum += loss.item()\n",
    "            batch_count += 1\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_d_loss = d_loss_sum / batch_count if batch_count > 0 else 0\n",
    "    avg_real_prob = real_probs_sum / real_count if real_count > 0 else 0\n",
    "    avg_fake_prob = fake_probs_sum / fake_count if fake_count > 0 else 0\n",
    "    \n",
    "    real_accuracy = avg_real_prob  # Average probability assigned to real samples\n",
    "    fake_accuracy = 1 - avg_fake_prob  # 1 - probability assigned to fake samples\n",
    "    overall_accuracy = (real_accuracy + fake_accuracy) / 2\n",
    "    \n",
    "    return {\n",
    "        'avg_d_loss': avg_d_loss,\n",
    "        'avg_real_prob': avg_real_prob,\n",
    "        'avg_fake_prob': avg_fake_prob,\n",
    "        'real_accuracy': real_accuracy,\n",
    "        'fake_accuracy': fake_accuracy,\n",
    "        'overall_accuracy': overall_accuracy\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Attempting to load target parameters...\n",
      "Loaded parameters of type: <class 'dict'>\n",
      "Failed to initialize with loaded parameters: cannot assign 'torch.FloatTensor' as parameter 'weight_ih_l0' (torch.nn.Parameter or None expected)\n",
      "Creating a fresh target LSTM...\n",
      "Created and saved new target parameters.\n",
      "Start pre-training discriminator...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'discriminator' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 403\u001b[0m\n\u001b[1;32m    400\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 227\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m y_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(y_batch)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    228\u001b[0m loss, probs \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_step(x_tensor, y_tensor)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Extract probabilities of being real\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'discriminator' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration parameters\n",
    "    EMB_DIM = 32  # Embedding dimension\n",
    "    HIDDEN_DIM = 32  # Hidden state dimension of LSTM cell\n",
    "    SEQ_LENGTH = 20  # Sequence length\n",
    "    START_TOKEN = 0\n",
    "    PRE_EPOCH_NUM = 20  # Supervised pre-training epochs\n",
    "    SEED = 88\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    # Discriminator parameters\n",
    "    dis_embedding_dim = 64\n",
    "    dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "    dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "    dis_dropout_keep_prob = 0.75\n",
    "    dis_l2_reg_lambda = 0.2\n",
    "    \n",
    "    # Training parameters\n",
    "    TOTAL_BATCH = 200\n",
    "    generated_num = 10000\n",
    "    \n",
    "    # File paths\n",
    "    os.makedirs('save', exist_ok=True)\n",
    "    positive_file = 'save/real_data.txt'\n",
    "    negative_file = 'save/generator_sample.txt'\n",
    "    eval_file = 'save/eval_file.txt'\n",
    "    \n",
    "    # Set random seeds\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize data loaders\n",
    "    gen_data_loader = GenDataLoader(BATCH_SIZE)\n",
    "    likelihood_data_loader = GenDataLoader(BATCH_SIZE)  # For testing\n",
    "    dis_data_loader = DisDataloader(BATCH_SIZE)\n",
    "    \n",
    "    # Vocabulary size\n",
    "    vocab_size = 5000\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator(\n",
    "        num_emb=vocab_size,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        emb_dim=EMB_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        sequence_length=SEQ_LENGTH,\n",
    "        start_token=START_TOKEN\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create or load the target LSTM (oracle)\n",
    "    try:\n",
    "        print(\"Attempting to load target parameters...\")\n",
    "        with open('save/target_params.pkl', 'rb') as f:\n",
    "            try:\n",
    "                target_params = pickle.load(f)\n",
    "                print(f\"Loaded parameters of type: {type(target_params)}\")\n",
    "                \n",
    "                # Initialize a fresh target LSTM\n",
    "                target_lstm = TargetLSTM(\n",
    "                    num_emb=vocab_size,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    emb_dim=EMB_DIM,\n",
    "                    hidden_dim=HIDDEN_DIM,\n",
    "                    sequence_length=SEQ_LENGTH,\n",
    "                    start_token=START_TOKEN\n",
    "                ).to(device)\n",
    "                \n",
    "                # Try to initialize with loaded parameters\n",
    "                try:\n",
    "                    target_lstm.init_model(target_params)\n",
    "                    print(\"Initialized target LSTM with loaded parameters.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to initialize with loaded parameters: {e}\")\n",
    "                    print(\"Creating a fresh target LSTM...\")\n",
    "                    target_lstm = TargetLSTM(\n",
    "                        num_emb=vocab_size,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        emb_dim=EMB_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        sequence_length=SEQ_LENGTH,\n",
    "                        start_token=START_TOKEN\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    # Save new parameters\n",
    "                    new_target_params = {\n",
    "                        'emb': target_lstm.embeddings.weight.data,\n",
    "                        'lstm': {\n",
    "                            'weight_ih_l0': target_lstm.lstm.weight_ih_l0.data,\n",
    "                            'weight_hh_l0': target_lstm.lstm.weight_hh_l0.data,\n",
    "                            'bias_ih_l0': target_lstm.lstm.bias_ih_l0.data,\n",
    "                            'bias_hh_l0': target_lstm.lstm.bias_hh_l0.data\n",
    "                        },\n",
    "                        'out_w': target_lstm.output_layer.weight.data,\n",
    "                        'out_b': target_lstm.output_layer.bias.data\n",
    "                    }\n",
    "                    \n",
    "                    with open('save/target_params.pkl', 'wb') as f:\n",
    "                        pickle.dump(new_target_params, f)\n",
    "                    print(\"Created and saved new target parameters.\")\n",
    "                    \n",
    "            except (UnicodeDecodeError, pickle.UnpicklingError) as e:\n",
    "                print(f\"Error unpickling parameters: {e}\")\n",
    "                print(\"Creating a fresh target LSTM...\")\n",
    "                target_lstm = TargetLSTM(\n",
    "                    num_emb=vocab_size,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    emb_dim=EMB_DIM,\n",
    "                    hidden_dim=HIDDEN_DIM,\n",
    "                    sequence_length=SEQ_LENGTH,\n",
    "                    start_token=START_TOKEN\n",
    "                ).to(device)\n",
    "                \n",
    "                # Save new parameters\n",
    "                new_target_params = {\n",
    "                    'emb': target_lstm.embeddings.weight.data,\n",
    "                    'lstm': {\n",
    "                        'weight_ih_l0': target_lstm.lstm.weight_ih_l0.data,\n",
    "                        'weight_hh_l0': target_lstm.lstm.weight_hh_l0.data,\n",
    "                        'bias_ih_l0': target_lstm.lstm.bias_ih_l0.data,\n",
    "                        'bias_hh_l0': target_lstm.lstm.bias_hh_l0.data\n",
    "                    },\n",
    "                    'out_w': target_lstm.output_layer.weight.data,\n",
    "                    'out_b': target_lstm.output_layer.bias.data\n",
    "                }\n",
    "                \n",
    "                with open('save/target_params.pkl', 'wb') as f:\n",
    "                    pickle.dump(new_target_params, f)\n",
    "                print(\"Created and saved new target parameters.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Target parameters file not found. Creating a new target LSTM...\")\n",
    "        target_lstm = TargetLSTM(\n",
    "            num_emb=vocab_size,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            sequence_length=SEQ_LENGTH,\n",
    "            start_token=START_TOKEN\n",
    "        ).to(device)\n",
    "        \n",
    "        # Save the parameters for future use\n",
    "        new_target_params = {\n",
    "            'emb': target_lstm.embeddings.weight.data,\n",
    "            'lstm': {\n",
    "                'weight_ih_l0': target_lstm.lstm.weight_ih_l0.data,\n",
    "                'weight_hh_l0': target_lstm.lstm.weight_hh_l0.data,\n",
    "                'bias_ih_l0': target_lstm.lstm.bias_ih_l0.data,\n",
    "                'bias_hh_l0': target_lstm.lstm.bias_hh_l0.data\n",
    "            },\n",
    "            'out_w': target_lstm.output_layer.weight.data,\n",
    "            'out_b': target_lstm.output_layer.bias.data\n",
    "        }\n",
    "        \n",
    "        os.makedirs('save', exist_ok=True)\n",
    "        with open('save/target_params.pkl', 'wb') as f:\n",
    "            pickle.dump(new_target_params, f)\n",
    "        print(\"Created and saved new target parameters.\")\n",
    "\n",
    "        # Initialize discriminator\n",
    "        try:\n",
    "            discriminator = Discriminator(\n",
    "                sequence_length=SEQ_LENGTH,\n",
    "                num_classes=2,\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=dis_embedding_dim,\n",
    "                filter_sizes=dis_filter_sizes,\n",
    "                num_filters=dis_num_filters,\n",
    "                l2_reg_lambda=dis_l2_reg_lambda,\n",
    "                dropout_keep_prob=dis_dropout_keep_prob\n",
    "            ).to(device)\n",
    "            print(\"Initialized discriminator successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing discriminator: {e}\")\n",
    "            raise  # Re-raise the exception to see the full error\n",
    "        \n",
    "        # Open log files\n",
    "        log = open('save/experiment-log.txt', 'w')\n",
    "        metrics_log = open('save/training-metrics.txt', 'w')\n",
    "        metrics_log.write('phase\\tepoch\\tg_loss\\tpre_d_loss\\tpre_real_prob\\tpre_fake_prob\\tpre_accuracy\\tpost_d_loss\\tpost_real_prob\\tpost_fake_prob\\tpost_accuracy\\n')\n",
    "        \n",
    "        # First, use the oracle model to generate real data\n",
    "        generate_samples(target_lstm, BATCH_SIZE, generated_num, positive_file, device)\n",
    "        gen_data_loader.create_batches(positive_file)\n",
    "        \n",
    "        # Pre-train the generator using MLE\n",
    "        print('Start pre-training generator...')\n",
    "        log.write('pre-training generator...\\n')\n",
    "        for epoch in range(PRE_EPOCH_NUM):\n",
    "            loss = pre_train_epoch(generator, gen_data_loader, device)\n",
    "            \n",
    "            if epoch % 5 == 0 or epoch == PRE_EPOCH_NUM - 1:\n",
    "                generate_samples(generator, BATCH_SIZE, generated_num, eval_file, device)\n",
    "                likelihood_data_loader.create_batches(eval_file)\n",
    "                test_loss = target_loss(target_lstm, likelihood_data_loader, device)\n",
    "                print(f'Pre-train epoch {epoch}, loss: {loss:.4f}, test_loss: {test_loss:.4f}')\n",
    "                buffer = f'epoch:\\t{epoch}\\tnll:\\t{test_loss}\\n'\n",
    "                log.write(buffer)\n",
    "                log.flush()\n",
    "    \n",
    "    # Pre-train the discriminator\n",
    "    print('Start pre-training discriminator...')\n",
    "    for d_pre_epoch in range(50):\n",
    "        generate_samples(generator, BATCH_SIZE, generated_num, negative_file, device)\n",
    "        dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "        \n",
    "        for inner_epoch in range(3):\n",
    "            dis_data_loader.reset_pointer()\n",
    "            \n",
    "            # Metrics tracking\n",
    "            d_loss_sum = 0.0\n",
    "            real_probs_sum = 0.0\n",
    "            fake_probs_sum = 0.0\n",
    "            real_count = 0\n",
    "            fake_count = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for _ in range(dis_data_loader.num_batch):\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                x_tensor = torch.LongTensor(x_batch).to(device)\n",
    "                y_tensor = torch.FloatTensor(y_batch).to(device)\n",
    "                \n",
    "                # Train discriminator\n",
    "                discriminator.train()\n",
    "                loss, probs = discriminator.train_step(x_tensor, y_tensor)\n",
    "                \n",
    "                # Extract probabilities of being real\n",
    "                real_probs = probs[:, 1].detach().cpu().numpy()\n",
    "                real_indices = np.where(y_batch[:, 1] == 1)[0]\n",
    "                fake_indices = np.where(y_batch[:, 0] == 1)[0]\n",
    "                \n",
    "                if len(real_indices) > 0:\n",
    "                    real_probs_sum += np.sum(real_probs[real_indices])\n",
    "                    real_count += len(real_indices)\n",
    "                \n",
    "                if len(fake_indices) > 0:\n",
    "                    fake_probs_sum += np.sum(real_probs[fake_indices])\n",
    "                    fake_count += len(fake_indices)\n",
    "                \n",
    "                d_loss_sum += loss\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if batch_count > 0:\n",
    "                avg_d_loss = d_loss_sum / batch_count\n",
    "                avg_real_prob = real_probs_sum / real_count if real_count > 0 else 0\n",
    "                avg_fake_prob = fake_probs_sum / fake_count if fake_count > 0 else 0\n",
    "                real_accuracy = avg_real_prob\n",
    "                fake_accuracy = 1 - avg_fake_prob\n",
    "                overall_accuracy = (real_accuracy + fake_accuracy) / 2\n",
    "                \n",
    "                # Log metrics\n",
    "                metrics_log.write(f'pre-train-d-epoch\\t{d_pre_epoch}\\t{inner_epoch}\\t-\\t{avg_d_loss:.4f}\\t{avg_real_prob:.4f}\\t{avg_fake_prob:.4f}\\t{overall_accuracy:.4f}\\n')\n",
    "                metrics_log.flush()\n",
    "                \n",
    "                # Print progress less frequently\n",
    "                if d_pre_epoch % 10 == 0 and inner_epoch == 2:\n",
    "                    print(f'Pre-train D epoch {d_pre_epoch}, inner epoch {inner_epoch}, loss: {avg_d_loss:.4f}')\n",
    "                    print(f'  Avg prob for real samples: {avg_real_prob:.4f}, fake samples: {avg_fake_prob:.4f}')\n",
    "                    print(f'  Real accuracy: {real_accuracy:.4f}, Fake accuracy: {fake_accuracy:.4f}, Overall: {overall_accuracy:.4f}')\n",
    "    \n",
    "    # Create rollout module\n",
    "    rollout = Rollout(generator, 0.8).to(device)\n",
    "    \n",
    "    print('#' * 80)\n",
    "    print('Start Adversarial Training...')\n",
    "    log.write('adversarial training...\\n')\n",
    "    \n",
    "    for total_batch in range(TOTAL_BATCH):\n",
    "        # Train the generator for one step\n",
    "        g_loss_sum = 0.0\n",
    "        \n",
    "        # Generate samples and get rewards\n",
    "        samples = generator.sample(BATCH_SIZE)\n",
    "        rewards = rollout.get_reward(samples, 16, discriminator)\n",
    "        \n",
    "        # Adversarial training step\n",
    "        generator.train()\n",
    "        g_loss = generator.adversarial_loss(samples, rewards)\n",
    "        g_loss_sum += g_loss\n",
    "        \n",
    "        avg_g_loss = g_loss_sum  # Since we only did one iteration\n",
    "        \n",
    "        # MEASURE POINT 1: Evaluate discriminator on new generator output before updating discriminator\n",
    "        generate_samples(generator, BATCH_SIZE, generated_num, negative_file, device)\n",
    "        dis_eval_loader = DisDataloader(BATCH_SIZE)\n",
    "        dis_eval_loader.load_train_data(positive_file, negative_file)\n",
    "        \n",
    "        pre_update_metrics = evaluate_discriminator(discriminator, dis_eval_loader, device, dis_dropout_keep_prob)\n",
    "        \n",
    "        # Test generator against the oracle model\n",
    "        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "            generate_samples(generator, BATCH_SIZE, generated_num, eval_file, device)\n",
    "            likelihood_data_loader.create_batches(eval_file)\n",
    "            test_loss = target_loss(target_lstm, likelihood_data_loader, device)\n",
    "            buffer = f'epoch:\\t{total_batch}\\tnll:\\t{test_loss}\\n'\n",
    "            print(f'Adversarial training epoch {total_batch}, test_loss: {test_loss:.4f}')\n",
    "            log.write(buffer)\n",
    "            log.flush()\n",
    "        \n",
    "        # Update rollout parameters\n",
    "        rollout.update_params()\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_loss_sum = 0.0\n",
    "        \n",
    "        for _ in range(5):  # Train discriminator for 5 rounds\n",
    "            generate_samples(generator, BATCH_SIZE, generated_num, negative_file, device)\n",
    "            dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "            \n",
    "            for inner_epoch in range(3):  # Train 3 epochs each round\n",
    "                dis_data_loader.reset_pointer()\n",
    "                \n",
    "                for _ in range(dis_data_loader.num_batch):\n",
    "                    x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                    x_tensor = torch.LongTensor(x_batch).to(device)\n",
    "                    y_tensor = torch.FloatTensor(y_batch).to(device)\n",
    "                    \n",
    "                    # Train discriminator\n",
    "                    discriminator.train()\n",
    "                    loss, _ = discriminator.train_step(x_tensor, y_tensor)\n",
    "                    d_loss_sum += loss\n",
    "        \n",
    "        # MEASURE POINT 2: Evaluate discriminator after it has been updated\n",
    "        dis_eval_loader.reset_pointer()\n",
    "        post_update_metrics = evaluate_discriminator(discriminator, dis_eval_loader, device, dis_dropout_keep_prob)\n",
    "        \n",
    "        # Log metrics\n",
    "        metrics_log.write(f'adv-train-epoch\\t{total_batch}\\t{avg_g_loss:.4f}\\t{pre_update_metrics[\"avg_d_loss\"]:.4f}\\t'\n",
    "                          f'{pre_update_metrics[\"avg_real_prob\"]:.4f}\\t{pre_update_metrics[\"avg_fake_prob\"]:.4f}\\t'\n",
    "                          f'{pre_update_metrics[\"overall_accuracy\"]:.4f}\\t{post_update_metrics[\"avg_d_loss\"]:.4f}\\t'\n",
    "                          f'{post_update_metrics[\"avg_real_prob\"]:.4f}\\t{post_update_metrics[\"avg_fake_prob\"]:.4f}\\t'\n",
    "                          f'{post_update_metrics[\"overall_accuracy\"]:.4f}\\n')\n",
    "        metrics_log.flush()\n",
    "    \n",
    "    # Close log files\n",
    "    log.close()\n",
    "    metrics_log.close()\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    # Save the trained models\n",
    "    torch.save(generator.state_dict(), 'save/generator.pt')\n",
    "    torch.save(discriminator.state_dict(), 'save/discriminator.pt')\n",
    "    \n",
    "    # Visualize training progress\n",
    "    plot_training_metrics('save/training-metrics.txt')\n",
    "\n",
    "def plot_training_metrics(metrics_file):\n",
    "    \"\"\"\n",
    "    Plot training metrics from the log file.\n",
    "    \n",
    "    Args:\n",
    "        metrics_file: Path to the metrics log file\n",
    "    \"\"\"\n",
    "    # Read metrics\n",
    "    df = pd.read_csv(metrics_file, sep='\\t')\n",
    "    \n",
    "    # Plot discriminator metrics during pre-training\n",
    "    pre_train_df = df[df['phase'] == 'pre-train-d-epoch']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(pre_train_df['epoch'], pre_train_df['pre_d_loss'])\n",
    "    plt.title('Pre-training Discriminator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(pre_train_df['epoch'], pre_train_df['pre_real_prob'], label='Real')\n",
    "    plt.plot(pre_train_df['epoch'], pre_train_df['pre_fake_prob'], label='Fake')\n",
    "    plt.title('Pre-training Discriminator Probabilities')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot generator and discriminator metrics during adversarial training\n",
    "    adv_train_df = df[df['phase'] == 'adv-train-epoch']\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(adv_train_df['epoch'], adv_train_df['g_loss'])\n",
    "    plt.title('Adversarial Training Generator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(adv_train_df['epoch'], adv_train_df['pre_accuracy'], label='Pre-update')\n",
    "    plt.plot(adv_train_df['epoch'], adv_train_df['post_accuracy'], label='Post-update')\n",
    "    plt.title('Discriminator Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('save/training_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
